{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/viksit-siddhant/speechClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, threading\n",
    "import numpy as np\n",
    "import random\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from models.convModel import ConvModel\n",
    "from utils import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"../data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torchaudio.load('data/LeNormand/TD/kevin.wav')[0]\n",
    "spec = torchaudio.transforms.Spectrogram(512)(audio)\n",
    "print(spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'rb') as f:\n",
    "    data = np.load(f)\n",
    "    czech_x = data['czech_x'].astype(np.float32)\n",
    "    czech_y = data['czech_y']\n",
    "    english_x = data['english_x'].astype(np.float32)\n",
    "    english_y = data['english_y']\n",
    "    lenormand_x = data['lenormand_x'].astype(np.float32)\n",
    "    lenormand_y = data['lenormand_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_x = np.concatenate((czech_x,english_x,lenormand_x),axis=0)\n",
    "combined_y = np.concatenate((czech_y,english_y,lenormand_y),axis=0)\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(combined_x,combined_y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14508, 1, 129, 129)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to 3 channels\n",
    "\n",
    "\n",
    "\n",
    "lenormand_neg_x = lenormand_x[lenormand_y.reshape((-1)) == 0]\n",
    "lenormand_neg_y = lenormand_y[lenormand_y.reshape((-1)) == 0]\n",
    "czech_neg_x = czech_x[czech_y.reshape((-1)) == 0]\n",
    "czech_neg_y = czech_y[czech_y.reshape((-1)) == 0]\n",
    "czech_pos_x = czech_x[czech_y.reshape((-1)) == 1]\n",
    "czech_pos_y = czech_y[czech_y.reshape((-1)) == 1]\n",
    "\n",
    "def inflate(x,y,target_len):\n",
    "    num_samples = max(len(x), target_len)\n",
    "    samples = np.random.randint(0, len(x), num_samples-len(x))\n",
    "    x = np.concatenate((x, x[samples]))\n",
    "    y = np.concatenate((y, y[samples]))\n",
    "    return x,y\n",
    "\n",
    "num_samples = max(len(lenormand_neg_x), len(czech_neg_x), len(czech_pos_x))\n",
    "lenormand_neg_x, lenormand_neg_y = inflate(lenormand_neg_x, lenormand_neg_y, num_samples)\n",
    "czech_neg_x, czech_neg_y = inflate(czech_neg_x, czech_neg_y, num_samples)\n",
    "czech_pos_x, czech_pos_y = inflate(czech_pos_x, czech_pos_y, 2*num_samples)\n",
    "\n",
    "train_data = Dataset(np.concatenate((lenormand_neg_x, czech_neg_x, czech_pos_x)), np.concatenate((lenormand_neg_y, czech_neg_y, czech_pos_y)))\n",
    "dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = m.get_data(dataloader)\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(feat, train_data.y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import activation\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_data = Dataset(train_x, train_y)\n",
    "test_data = Dataset(test_x, test_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "convmod = ConvModel(train_x.shape[1:])\n",
    "\n",
    "opt = torch.optim.Adam(convmod.parameters(), lr=0.001)\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0,translate=(0.66,0.66)),\n",
    "    transforms.GaussianBlur(3, sigma=(0.1, 0.6)),\n",
    "    ])\n",
    "\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "def train(model, train_loader, test_loader, loss, opt, epochs,transformer = None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Loading model to \", device)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_steps = len(train_loader)\n",
    "        counter = 0\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device,dtype=torch.float)\n",
    "            y = y.to(device,dtype=torch.float)\n",
    "          #  y = torch.flatten(y)\n",
    "            x = transformer(x)\n",
    "            opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            l = loss(pred, y)\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "            train_loss += l.item()\n",
    "            print(f\"Step {counter+1} of {train_steps}\", end='\\r')\n",
    "            counter+=1\n",
    "        print(\"\")\n",
    "        print(\"Train Loss: \",train_loss)\n",
    "        model.eval()\n",
    "        if test_loader is None:\n",
    "            continue\n",
    "        test_loss = 0\n",
    "        counter = 0\n",
    "        num_correct_predictions = 0\n",
    "        test_steps = len(test_loader)\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x = x.to(device,dtype=torch.float)\n",
    "                y = y.to(device,dtype=torch.float)\n",
    "               # y = torch.flatten(y)\n",
    "                pred = model(x)\n",
    "                l = loss(pred, y)\n",
    "                y = torch.flatten(y)\n",
    "                test_loss += l.item()\n",
    "\n",
    "                pred = torch.argmax(pred, dim=1)\n",
    "                num_correct_predictions += torch.sum(pred == y).item()\n",
    "                print(f\"Step {counter+1} of {test_steps}\", end='\\r')\n",
    "                counter+=1\n",
    "        print(\"\")\n",
    "        print(f\"Test Loss: {test_loss}, Accuracy: {num_correct_predictions/len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model to  cuda\n",
      "Epoch 1 of 10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 1.95 GiB total capacity; 760.27 MiB already allocated; 60.19 MiB free; 778.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#print(torch.cuda.is_available())\u001b[39;00m\n\u001b[1;32m      2\u001b[0m convmod\u001b[38;5;241m.\u001b[39munfreeze()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, loss, opt, epochs, transformer)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m transformer(x)\n\u001b[1;32m     36\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(pred, y)\n\u001b[1;32m     39\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Projects/speechClass/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/speechClass/models/convModel.py:21\u001b[0m, in \u001b[0;36mConvModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m---> 21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mReLU()(x)\n\u001b[1;32m     23\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m~/Projects/speechClass/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/speechClass/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Projects/speechClass/lib/python3.8/site-packages/torch/nn/modules/conv.py:439\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    437\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    438\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 439\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    440\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 1.95 GiB total capacity; 760.27 MiB already allocated; 60.19 MiB free; 778.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#print(torch.cuda.is_available())\n",
    "convmod.unfreeze()\n",
    "train(convmod, train_loader, test_loader, loss, opt, 10, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot 10 random images from model.train_x\n",
    "\n",
    "figs, axs = plt.subplots(2,5, figsize=(15,6))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        axs[i,j].imshow(train_data[random.randint(0,len(train_data))][0][0], cmap='viridis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenormand = LeNormandData(16000, 32,32)\n",
    "#print(model.train_x[0])\n",
    "\n",
    "#Plot 10 random images from model.train_x\n",
    "\n",
    "figs, axs = plt.subplots(2,5, figsize=(15,6))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        axs[i,j].imshow(lenormand[np.random.randint(0,len(lenormand))][0].squeeze())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.1\n",
    "train_ln,test_ln = torch.utils.data.random_split(lenormand,[int(train_fraction*len(lenormand)),len(lenormand)-int(train_fraction*len(lenormand))])\n",
    "train_ln = torch.utils.data.DataLoader(train_ln, batch_size=32, shuffle=True)\n",
    "test_ln = torch.utils.data.DataLoader(test_ln, batch_size=32, shuffle=True)\n",
    "convmod.freeze(1)\n",
    "\n",
    "train(convmod, train_ln, test_ln, loss, opt, 20, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_loader = torch.utils.data.DataLoader(lenormand, batch_size=32, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_correct_0 = 0\n",
    "num_correct_1 = 0\n",
    "num_0 = 0\n",
    "num_1 = 0\n",
    "for x,y in ln_loader:\n",
    "    x = x.to(device,dtype=torch.float)\n",
    "    y = y.numpy().flatten()\n",
    "    with torch.no_grad():\n",
    "        pred = np.argmax(convmod(x).cpu().numpy(),axis=1)\n",
    "        num_correct_0 += np.sum((pred < 0.5) & (y == 0))\n",
    "        num_correct_1 += np.sum((pred >= 0.5) & (y == 1))\n",
    "        print(pred)\n",
    "        print(y)\n",
    "        num_0 += np.sum(y == 0)\n",
    "        num_1 += np.sum(y == 1)\n",
    "\n",
    "print(f\"Accuracy on zero: {num_correct_0/num_0}\")\n",
    "print(f\"Accuracy on one: {num_correct_1/num_1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('speechClass': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b91e72974164e2d01c3ab4c2fedc06e339e111d9a2a09c8eaeaab510c7412d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
